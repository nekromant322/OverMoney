  server:
    port: 8080

  integration:
    internal:
      host:
        orchestrator: http://orchestrator-service
        wit-ai-proxy: http://wit-ai-go-proxy-service

  environment: ${ENVIRONMENT:dev}

  spring:
    application:
      name: recognizer-service

    profiles:
      active: ${SPRING_APPLICATION_PROFILE:dev}

  logging:
    level:
      # Уровень логирования для всех Feign-клиентов
      feign.Logger: DEBUG
      feign.Client: DEBUG
      feign.Request: DEBUG
      feign.Response: DEBUG
      # Пакет, где находятся твои Feign-клиенты — замени на свой
      com.override.recognizer_service.feign: DEBUG

  management:
    endpoints:
      web:
        exposure:
          include: health,prometheus,gitInfo
    metrics:
      export:
        prometheus:
          enabled: true
      distribution:
        percentiles-histogram:
          "[http.server.requests]": true

  authorization-header:
    header-value: ${INTERNAL_KEY_HEADER:X-INTERNAL-KEY}

  mask-log-spring-boot-starter:
    maskedFields:
      - x-internal-key
      - voiceMessageBytes

  llm:
    model: "llama3.2:latest"
    url: "http://195.133.49.108:11434/api/chat"
    options:
      temperature: 0.2
      top_p: 0.9
      repetition_penalty: 1.1

    limit:
      keywords: 3
      verbose-categories: 8

  recognizer:
    llm-algo: ACTIVE
    levenshtein-algo: ACTIVE



